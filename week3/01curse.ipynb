{
   "metadata" : {
      "org" : null,
      "kernelspec" : {
         "language" : "python",
         "display_name" : "Python 3",
         "name" : "python3"
      },
      "language_info" : {
         "nbconvert_exporter" : "python",
         "file_extension" : ".py",
         "codemirror_mode" : {
            "version" : 3,
            "name" : "ipython"
         },
         "mimetype" : "text/x-python",
         "version" : "3.5.2",
         "name" : "python",
         "pygments_lexer" : "ipython3"
      }
   },
   "nbformat" : 4,
   "cells" : [
      {
         "source" : [
            "## The curse of dimensionality\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "We&rsquo;ll consider various metric properties of $\\mathbb{R}^n$.  We&rsquo;ll\nfind that there&rsquo;s more space than you might think in higher\ndimensions, which should serve as a warning to you: when your data\nsits in some high-dimensional space, you are in danger of having\nsparser data than you might expect!  This is the **curse of\ndimensionality** and it motivates various dimension-reduction\nprocedures that we&rsquo;ll be studying this week.\n\nThe story we tell here &#x2013; particularly that the volume of the $n$-ball\nreaches a maximum (!) &#x2013; is not only an important observation for data\nscience but also for geometry and, well, for mathematics.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "source" : [
            "## The volume of a hypersphere\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "The volume of the $n$-ball can be computed as follows.\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "code",
         "outputs" : [],
         "source" : [
            "import numpy as np\nfrom scipy.special import gamma\n\ndef volume_ball(n,r):\n    return (np.pi ** (n/2)) * (r**n) / gamma( 1 + n/2 )"
         ],
         "execution_count" : 1
      },
      {
         "source" : [
            "But this is a data science course, so let&rsquo;s see if we can &ldquo;check&rdquo; this\nformula via some monte carlo method.\n\n****Warning:**** I&rsquo;ve purposefully included a typo in the code below.  Can\nyou catch it and fix the bug?q\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "execution_count" : 1,
         "source" : [
            "import numpy as np\n\ndef volume_ball_mc(n,r):\n    count = 100000\n    points = np.random.uniform( -r, r, size=(count,n) )\n    cube_volume = (2*r)**n\n    # There is a bug somewhere in this code!  Find it, fix it!\n    sphere_to_cube = np.sum(np.linalg.norm(points) < r) / count\n    return cube_volume * sphere_to_cube"
         ],
         "metadata" : {},
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "source" : [
            "Our definition of `volume_ball` involved `np.pi` but there&rsquo;s no $\\pi$\nin `volume_ball_mc`.  Let&rsquo;s estimate it!  This also serves as a quick\ncheck that you fixed the bug.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "print(volume_ball_mc(2,1))\nprint(volume_ball_mc(3,1)*3/4)"
         ],
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "****Plot these**** as functions of $n$.\n\nFor which $n$ is `volume_ball(n,1)` the largest?\n\nIn `volume_ball_mc`, play with `count` and consider say `n = 20`.  How\nlikely is a point in a high-dimensional cube to also be in a\nhigh-dimensional sphere?\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## Distance to nearest neighbors\n\n"
         ]
      },
      {
         "source" : [
            "Let&rsquo;s again consider randomly chosen points in $\\mathbb{R}^n$.  How\nfar do we expect each point is to nearby neighbors?\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "code",
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "import scipy.spatial\n\ndef nearest_neighbor_distances(points, k):\n    neighbors = []\n    tree = scipy.spatial.cKDTree(points)\n    dd, ii = tree.query(points, k=range(2,2+k))\n    return dd\n\ndef average_knn_distance(n):\n    count = 10000\n    points = np.random.uniform( -1, 1, size=(count,n) )\n    return np.mean( nearest_neighbor_distances( points, 1 ) )"
         ],
         "execution_count" : 1
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "Plot `average_knn_distance` for various values of `n`.  Does this\nworry you at all?  Even when $n$ isn&rsquo;t **that** large, $\\mathbb{R}^n$ is\na lonely place.  When we do clustering, we&rsquo;ll often care about\npairwise distances, so we must face this problem.  One way out is\nthrough various **dimensionality reduction** algorithms.\n\n"
         ]
      }
   ],
   "nbformat_minor" : 0
}
