{
   "nbformat" : 4,
   "cells" : [
      {
         "source" : [
            "## Computing SVD with numpy\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Let&rsquo;s pick a matrix at random.\n\n"
         ],
         "metadata" : {}
      },
      {
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code",
         "source" : [
            "import numpy as np\nm = 6\nn = 5\nA = np.random.normal(0,1,size=(m,n))"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "Note that $A$ is **not** square, so `np.linalg.eigvals` fails.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "np.linalg.eigvals(A)"
         ],
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code"
      },
      {
         "metadata" : {},
         "source" : [
            "Nevertheless, we can compute the singular-value decomposition, usually\nknown by the acronym SVD.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "U, s, V = np.linalg.svd(A)"
         ],
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code"
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Now $A$ is $m \\times n$, or in `numpy`&rsquo;s syntax, `A.shape == (m,n)`.\n\nThen $U$ is $m \\times m$, and $s$ has length $\\mathrm{min}(m,n)$, and\n$V$ is $n \\times n$.  Let&rsquo;s check this.\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "code",
         "execution_count" : 1,
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "U.shape == (m,m) and len(s) == min(m,n) and V.shape == (n,n)"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "The numbers in `s` are the **singular values** of $A$.  The columns of\n$U$ are the **left-singular vectors** and the columns of $V$ are the\n**right-singular vectors** of $A$.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## SVD is a matrix factorization\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "There are many &ldquo;matrix factorizations&rdquo; in the world.  SVD is such a\nfactorization.  Let&rsquo;s see this in some specific case.\n\n"
         ]
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code",
         "metadata" : {},
         "source" : [
            "import numpy as np\nm = 6\nn = 5\nA = np.random.normal(0,1,size=(m,n))\n\nU, s, V = np.linalg.svd(A)\n\nS = np.zeros( (m, n) )\nS[:len(s), :len(s)] = np.diag(s)\n\nnp.linalg.norm( U.dot(S.dot(V)) - A )"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "So $A$ is (well, nearly &#x2013; we&rsquo;re working with floats after all!) the\nproduct $USV$ where $S$ is a matrix build from the singular values\n$s$.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "## SVD as a \"numerical rank\"\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "One application of SVD is as a more stable way to calculate rank.\n\nLet&rsquo;s build a low-rank matrix.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "code",
         "execution_count" : 1,
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "import numpy as np\nm = 6\nn = 5\nr = 2\nX = np.random.normal(0,1,size=(m,r))\nY = np.random.normal(0,1,size=(r,n))\nA = X.dot(Y)"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "We can check that this is indeed low-rank.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "code",
         "execution_count" : 1,
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "np.linalg.matrix_rank(A)"
         ]
      },
      {
         "source" : [
            "Now if we add some noise to $A$, we might begin to worry.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "noise = np.random.normal(0,0.01,size=(m,n))\nnp.linalg.matrix_rank(A + noise)"
         ],
         "metadata" : {},
         "cell_type" : "code",
         "outputs" : [],
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "The rank of $A + \\mathrm{noise}$ is indeed $\\mathrm{min}(m,n)$.\n\nLook at the singular values though.\n\n"
         ]
      },
      {
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code",
         "source" : [
            "_, s, _ = np.linalg.svd(A + noise)\ns"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "There are some large singular values, and some small singular values.\nLet&rsquo;s leverage this observation to compute rank even for noisy\nmatrices.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "def numerical_rank(A, epsilon=0.1):\n   _, s, _ = np.linalg.svd(A + noise)\n   return np.sum( np.abs(s) > epsilon )"
         ],
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code"
      },
      {
         "cell_type" : "code",
         "execution_count" : 1,
         "outputs" : [],
         "source" : [
            "numerical_rank(A)"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## SVD for the pseudoinverse\n\n"
         ],
         "metadata" : {}
      },
      {
         "metadata" : {},
         "source" : [
            "Let&rsquo;s again pick a random matrix $A$ and finds its SVD.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "import numpy as np\nm = 6\nn = 5\nA = np.random.normal(0,1,size=(m,n))\n\nU, s, V = np.linalg.svd(A)"
         ],
         "metadata" : {},
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code"
      },
      {
         "metadata" : {},
         "source" : [
            "Now construct $A^{+}$, a **pseudoinverse** of $A$.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code",
         "metadata" : {},
         "source" : [
            "D = np.zeros( (n, m) )\nD[:len(s), :len(s)] = np.diag(1/s)\n\nAplus = (V.T).dot( D.dot( U.T ) )\nAplus.dot( A )"
         ]
      },
      {
         "source" : [
            "Is this ever the actual inverse of $A$?  When $A$ is not square, what\nare some nevertheless nice properties of the pseudoinverse?\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## SVD for dimension reduction\n\n"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "Given a matrix $A$, we could ask for the &ldquo;nearest&rdquo; matrix to $A$ which\nhas rank $r$.\n\nYour homework is to structure this as yet another &ldquo;cost&rdquo; problem,\ni.e., the cost to minimize is the sum of the squares of the difference\nof the entries of $A$ and the rank $r$ approximation.  Randomly sample to get a sense of the distribution of these cost functions.\n\nThen see how well you do with SVD; throw away all but the largest $r$\nsingular values, replace the others with zero, and perform the same\n&ldquo;matrix reconstruction&rdquo; as above.  Of course, you won&rsquo;t get the\noriginal matrix, but what is the rank of the matrix you produced?  How\nclose is it to the original matrix?\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "## SVD to PCA\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "There is a lot more to say about the power of SVD.\n\nOne thing to dig into is the relationship between SVD and PCA.  Some of this is discussed at [https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca](https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca)\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## Underview\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Your comparative advantage in the marketplace lies in your ability to\nleverage your existing knowledge and apply it to data science.  With\nthings like SVD, I hope you recognize just how strong you really are:\nyou know a ton of mathematics, and you can use this to dig deeper &#x2013;\nand dig quickly &#x2013; into data science.  The intuition you have from\nlinear algebra will serve you well throughout your career.\n\n"
         ]
      }
   ],
   "metadata" : {
      "language_info" : {
         "codemirror_mode" : {
            "name" : "ipython",
            "version" : 3
         },
         "mimetype" : "text/x-python",
         "file_extension" : ".py",
         "nbconvert_exporter" : "python",
         "name" : "python",
         "pygments_lexer" : "ipython3",
         "version" : "3.5.2"
      },
      "org" : null,
      "kernelspec" : {
         "display_name" : "Python 3",
         "name" : "python3",
         "language" : "python"
      }
   },
   "nbformat_minor" : 0
}
