{
   "metadata" : {
      "org" : null,
      "language_info" : {
         "file_extension" : ".py",
         "nbconvert_exporter" : "python",
         "mimetype" : "text/x-python",
         "codemirror_mode" : {
            "version" : 3,
            "name" : "ipython"
         },
         "pygments_lexer" : "ipython3",
         "name" : "python",
         "version" : "3.5.2"
      },
      "kernelspec" : {
         "language" : "python",
         "name" : "python3",
         "display_name" : "Python 3"
      }
   },
   "nbformat_minor" : 0,
   "nbformat" : 4,
   "cells" : [
      {
         "metadata" : {},
         "source" : [
            "I should mention that this technique (latent semantic analysis,\notherwise known as LSA) is related to principal component analysis\n(PCA).  Matt Osborne will be presenting much more on PCA during a\nspecial session.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "## Load some real-world data\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "Let&rsquo;s see how this might work by reprising our reddit data set.\n\n"
         ]
      },
      {
         "outputs" : [],
         "metadata" : {},
         "cell_type" : "code",
         "source" : [
            "import json\nimport bz2\ncomments = []\nwith bz2.open('/home/jim/downloads/RC_2010-10.bz2', 'r') as f:\n    for line in f:\n        comment = json.loads(line.strip().decode('utf-8'))\n        if comment['subreddit'] == 'politics':\n            if comment['body'] != '[deleted]':\n                comments.append( comment )\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ncorpus = [comment['body'] for comment in comments]\nX = vectorizer.fit_transform(corpus)"
         ],
         "execution_count" : 1
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "At this point `X` is rows of vectors for each &ldquo;document&rdquo; which in this\ncase is a reddit comment.\n\n"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "## Reduce dimension\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "execution_count" : 1,
         "metadata" : {},
         "cell_type" : "code",
         "source" : [
            "from sklearn.decomposition import  TruncatedSVD\ntsvd = TruncatedSVD(n_components=300)\ntsvd.fit(X)  \nX2 = tsvd.transform(X)"
         ],
         "outputs" : []
      },
      {
         "metadata" : {},
         "source" : [
            "You might enjoy changing `n_components`.  In this case, `300` is a\n&ldquo;recommended number.&rdquo;\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "## Then cluster\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "Why is it reasonable (or a good idea) to first perform SVD before\nclustering?\n\n"
         ]
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code",
         "source" : [
            "from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=10).fit(X2)\nkmeans.labels_"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Explore the clusters\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "metadata" : {},
         "source" : [
            "for j in np.unique(kmeans.labels_):\n    print(\"****************************************************************\")\n    print(\"Cluster\",j)\n    for i in np.random.choice( np.nonzero(kmeans.labels_ == j)[0], size=20, replace=False ):\n        print(corpus[i][0:70])"
         ],
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Homework\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Given a document, find (and print) documents which are nearby in the\n&ldquo;semantic space&rdquo; computed by SVD.\n\n"
         ],
         "metadata" : {}
      }
   ]
}
